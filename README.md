# AskMyNotes

This repository contains the backend API for the AskMyNotes application. The service is built with FastAPI and provides user authentication as well as a user profile feature where each user can store three subject names.

## Available endpoints

### Authentication
- `POST /auth/register` – register a new user (name, email, password).
- `POST /auth/login` – obtain a JWT token by providing email and password.
- `GET /auth/me` – retrieve information about the currently authenticated user.

### Profile
- `POST /profile/` – create or update the current user's profile. The payload must include exactly three subjects:
  ```json
  {
      "subject1": "Mathematics",
      "subject2": "Physics",
      "subject3": "Chemistry"
  }
  ```
- `GET /profile/` – fetch the current user's profile (requires valid JWT in Authorization header).

API documentation is automatically generated by FastAPI and available at `/docs` when the server is running.

> **Embedding dimension:** the system uses a 1536‑dimensional embedding
> vector by default (matching OpenAI's `text-embedding-3-small`).
> You can override it with the `EMBEDDING_DIM` environment variable, but
> make sure it matches the model you configure via `OPENAI_EMBEDDING_MODEL`.
The project now uses the OpenAI Python SDK to generate embeddings by
default when an `OPENAI_API_KEY` is present. It is currently **locked to the
`text-embedding-3-small`** model (1536‑dimensional); any attempt to change
the model via `OPENAI_EMBEDDING_MODEL` will raise an error. During local
development the code will fall back to a deterministic mock provider if the
key is missing. Set `OPENAI_API_KEY` in your environment before starting the
app for real embeddings. You can verify which provider/model is active by
watching the application log during startup; the server logs an informative
message once it finishes booting.

### Document storage and retrieval

- `POST /doc/upload` – upload a PDF file along with a subject name. The
  server splits the document into chunks, generates embeddings, and stores
  both the text and vectors in a Qdrant collection. Each chunk is tagged with
  the uploading user's ID and the associated subject; per-user isolation is
  enforced at query time.

- `POST /doc/query` – retrieve an answer to a question using only your own
  documents.  Example payload:
  ```json
  {
      "subject": "Mathematics",
      "question": "What is the Pythagorean theorem?",
      "top_k": 5
  }
  ```

  The service performs the following RAG flow (the underlying LLM can be
configured with the ``LLM_MODEL`` environment variable; it defaults to
``gpt-3.5-turbo``):
  1. Generate an embedding for the question using the same provider as during
     upload.
  2. Search Qdrant for the top-k chunks that match the vector, filtering by
     both `user_id` and `subject` so that other users' data is never
     considered.
  3. Build a context string from the retrieved chunks (including document
     name and chunk index) and send it, along with the original question, to
     a language model.
  4. The model is instructed to answer *only* from the provided context and
     to reply with "I don't know" if the information isn't present.  The raw
     model call is encapsulated in the `doc_chunk_store.llm` module so the
     router remains thin.

  The HTTP response contains the final `answer` string produced by the LLM
  and a `sources` list showing which document chunks were used.

> **Note:** The document ingestion endpoints depend on a reachable Qdrant
> vector database. You can either:
> 
> * **Run locally** using Docker (`docker run -p 6333:6333 qdrant/qdrant`).
> * **Use a managed cluster** – set `QDRANT_URL` to the full HTTPS endpoint and
>   `QDRANT_API_KEY` to the provided API key.
> 
> The legacy `QDRANT_HOST`/`QDRANT_PORT` settings remain supported for simple
> local deployments. If the application cannot connect to Qdrant, upload/query
> calls will return a 503 Service Unavailable with an explanatory message. 

**Integration & Local Development**

- **Backend base path:** The API is mounted under `/api` by default. Example auth endpoints are available at `/api/auth/register`, `/api/auth/login` and `/api/auth/me`.
- **CORS:** The backend reads `FRONTEND_ORIGINS` (comma-separated) to allow origins. By default it allows `http://localhost:8080` for local frontend development.
- **Frontend:** The frontend reads `VITE_API_BASE` and `VITE_API_URL` from environment files. Development env is in `frontend/.env.development` (defaults to `VITE_API_BASE=/api` and `VITE_API_URL=http://localhost:8000`). Vite is configured to proxy `/api` to the backend during `npm run dev`.
- **One-step dev (Windows):** Run `dev.cmd` from the repository root to open two terminal windows — one for the backend (activates `neossis_env`) and one for the frontend. You can alternatively run the services manually:

```powershell
# Backend (from repository root)
neossis_env\Scripts\activate  # activate your venv on Windows
pip install -r backend/requirements.txt
cd backend
uvicorn main:app --reload --port 8000

# Frontend (new terminal)
cd frontend
npm install
npm run dev
```

- **Health check:** Backend health endpoint is available at `http://localhost:8000/api/health`.

- **Chat requests:** When using the chat interface on any subject card, the frontend sends a POST to `http://localhost:8000/api/doc/query` with
  ```json
  { "subject": "<subject name>", "question": "<user query>", "top_k": 5 }
  ```
  The request includes the JWT token from `askynotes_token` in the Authorization header; responses are displayed with citations and are stored in the chat history. Ensure your backend is running and the token is valid to receive answers.

- **Theme:** A light/dark toggle exists in the navbar. It uses `next-themes` and persists your preference. The UI supports both modes via Tailwind `dark` class and CSS token overrides.
- **Visuals:** Gradients have been removed in favor of solid colors; the application no longer uses the `gradient-text` or `gradient-border` utility classes. The home/dashboard page spacing has been tightened to make the layout denser and less empty.

- **Voice agent:** A lightweight Node/Express service provides speech‑to‑text, question answering (via the main Python backend), and text‑to‑speech. The service lives in `backend/server.js` and runs on port `5001` by default. To start it you'll need a valid `OPENAI_API_KEY` (for Whisper/tts) and optionally `PYTHON_API_BASE` if your FastAPI server is running on a different host. Use `npm install` inside `backend/` and `npm run dev` during development. 

  **Troubleshooting:** ensure the voice server is running before opening the frontend. The Vite dev server proxies `/voice` requests to `VITE_VOICE_URL` (default `http://localhost:5001`). If the backend is down you will see proxy errors and a 500 response; the frontend now shows a toast explaining the service is unreachable. Do **not** start the voice backend on port 8080 (that port is reserved for the Vite dev server). A `PORT` environment variable allows you to change the voice service port if needed.

  The chat interface on study pages includes a microphone button — press it to record, stop to send, then hear a spoken reply. Transcripts are shown in the UI and all audio is streamed via the `/voice` proxy.

- **Production:** In production set `FRONTEND_ORIGINS` to your frontend origin(s) and set `VITE_API_URL` to the production API URL. Ensure you serve the frontend built assets from a static host and point the API URL accordingly.
