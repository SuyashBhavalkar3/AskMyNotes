"""Lightweight wrapper around a large language model.

The goal of this module is to keep all LLM-related code in one place so the
business logic in :mod:`services` can remain clean.  Currently it uses the
OpenAI ChatCompletion API via the v2 SDK, but the rest of the application
simply calls :func:`LLMService.answer_question` and doesn't need to know which
provider is being used.

The service is intentionally simplistic; production users will likely want to
add retries, caching, concurrency limits, etc.  For the purposes of the
hackathon, the implementation below satisfies the requirement to answer a
question based solely on supplied context and to politely admit when the
information isn't available.
"""

import os
from dotenv import load_dotenv

from openai import OpenAI

# make sure environment variables (API key, model names, etc.) are loaded
load_dotenv()

import logging

# log which model will be used for completions; this mirrors the embedding
# provider's startup message and gives visibility during development.
logging.getLogger(__name__).info("LLM model set to %s", os.getenv("LLM_MODEL", "gpt-3.5-turbo"))


class LLMService:
    @staticmethod
    def answer_question(question: str, context: str) -> str:
        """Return an answer generated by the configured LLM.

        ``question`` is the userâ€‘supplied query, and ``context`` contains the
        concatenated text chunks retrieved from Qdrant.  The context may be an
        empty string if no matching documents were found; callers should
        generally handle that case before invoking the model.

        The prompt sent to the model includes a strong system message that
        instructs the model to **only** use the provided context.  If the
        answer cannot be determined from the context, the model is asked to
        reply with a clear "I don't know" message rather than hallucinating.
        """

        if context is None:
            context = ""

        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("OPENAI_API_KEY must be set to use LLMService")

        model = os.getenv("LLM_MODEL", "gpt-3.5-turbo")

        client = OpenAI(api_key=api_key)

        # build a chat request; temperature is set to 0 to minimise
        # invention/hallucination.
        system_msg = (
            "You are a helpful assistant.\n"
            "Answer the user's question using ONLY the provided context.\n"
            "Do NOT use any outside knowledge or make assumptions.\n"
            "If the answer cannot be found in the context, respond with a clear\n"
            "statement such as 'I don't know' or 'The information is not available'.\n"
        )
        user_msg = f"Context:\n{context}\n\nQuestion: {question}"

        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            temperature=0.0,
            max_tokens=500,
        )

        # the OpenAI SDK returns a list of choices; we always take the first one
        answer = resp.choices[0].message.content.strip()
        return answer
